{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e21c15b",
   "metadata": {
    "papermill": {
     "duration": 0.003779,
     "end_time": "2025-01-20T03:02:01.378890",
     "exception": false,
     "start_time": "2025-01-20T03:02:01.375111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "####  Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8f95b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:01.386890Z",
     "iopub.status.busy": "2025-01-20T03:02:01.386562Z",
     "iopub.status.idle": "2025-01-20T03:02:01.391832Z",
     "shell.execute_reply": "2025-01-20T03:02:01.391238Z"
    },
    "papermill": {
     "duration": 0.010747,
     "end_time": "2025-01-20T03:02:01.393059",
     "exception": false,
     "start_time": "2025-01-20T03:02:01.382312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "requirements_content = \"\"\"\n",
    "torch==2.1.2\n",
    "torchvision==0.16.2\n",
    "torchaudio==2.1.2\n",
    "pytorch-lightning==2.1.3\n",
    "ray[train]==2.9.0\n",
    "deepspeed==0.12.3\n",
    "tiktoken==0.5.2\n",
    "pandas==2.1.4\n",
    "numpy==1.24.3\n",
    "pyyaml==6.0.1\n",
    "omegaconf==2.3.0\n",
    "dataclasses==0.6\n",
    "typing-extensions==4.9.0\n",
    "tensorboard==2.15.1\n",
    "pytest==7.4.4\n",
    "black==23.12.1\n",
    "isort==5.13.2\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0151ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:01.400191Z",
     "iopub.status.busy": "2025-01-20T03:02:01.399962Z",
     "iopub.status.idle": "2025-01-20T03:02:01.402687Z",
     "shell.execute_reply": "2025-01-20T03:02:01.402081Z"
    },
    "papermill": {
     "duration": 0.007588,
     "end_time": "2025-01-20T03:02:01.403842",
     "exception": false,
     "start_time": "2025-01-20T03:02:01.396254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt install python3.10-venv\n",
    "# !python -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# !pip install -r /kaggle/working/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e51b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:01.410842Z",
     "iopub.status.busy": "2025-01-20T03:02:01.410612Z",
     "iopub.status.idle": "2025-01-20T03:02:01.413271Z",
     "shell.execute_reply": "2025-01-20T03:02:01.412709Z"
    },
    "papermill": {
     "duration": 0.007396,
     "end_time": "2025-01-20T03:02:01.414382",
     "exception": false,
     "start_time": "2025-01-20T03:02:01.406986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f031381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:01.421524Z",
     "iopub.status.busy": "2025-01-20T03:02:01.421335Z",
     "iopub.status.idle": "2025-01-20T03:02:24.633534Z",
     "shell.execute_reply": "2025-01-20T03:02:24.632496Z"
    },
    "papermill": {
     "duration": 23.217413,
     "end_time": "2025-01-20T03:02:24.635127",
     "exception": false,
     "start_time": "2025-01-20T03:02:01.417714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\r\n",
      "  Downloading deepspeed-0.16.2.tar.gz (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: ray[train] in /usr/local/lib/python3.10/dist-packages (2.40.0)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[train]) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[train]) (3.16.1)\r\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[train]) (4.23.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[train]) (1.1.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[train]) (24.2)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[train]) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[train]) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[train]) (1.3.2)\r\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[train]) (1.5.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[train]) (2.32.3)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[train]) (2.2.2)\r\n",
      "Collecting tensorboardX>=1.9 (from ray[train])\r\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[train]) (17.0.0)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[train]) (2024.9.0)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\r\n",
      "Collecting hjson (from deepspeed)\r\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.3)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\r\n",
      "Collecting nvidia-ml-py (from deepspeed)\r\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[train]) (24.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[train]) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[train]) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[train]) (0.22.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[train]) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[train]) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[train]) (2024.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[train]) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[train]) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[train]) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[train]) (2024.12.14)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[train]) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deepspeed) (2024.2.0)\r\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\r\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.16.2-py3-none-any.whl size=1548023 sha256=158ae54521ed946cbb26dc289e69762c2f03be9b80e6d1be8a5ac67a9d78626c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/67/38/c71324ccf43b5634ac1b65033fe8bb9a13a9dfa2dad4eee940\r\n",
      "Successfully built deepspeed\r\n",
      "Installing collected packages: nvidia-ml-py, hjson, tensorboardX, deepspeed\r\n",
      "Successfully installed deepspeed-0.16.2 hjson-3.1.0 nvidia-ml-py-12.560.30 tensorboardX-2.6.2.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"ray[train]\"  deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438e3ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:24.645246Z",
     "iopub.status.busy": "2025-01-20T03:02:24.644988Z",
     "iopub.status.idle": "2025-01-20T03:02:33.227289Z",
     "shell.execute_reply": "2025-01-20T03:02:33.226571Z"
    },
    "papermill": {
     "duration": 8.588915,
     "end_time": "2025-01-20T03:02:33.228894",
     "exception": false,
     "start_time": "2025-01-20T03:02:24.639979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, Optional\n",
    "import ray\n",
    "from ray.air import RunConfig, ScalingConfig  # Updated import\n",
    "from ray.train.lightning import RayLightningEnvironment, prepare_trainer\n",
    "import torch\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e3a94e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:33.239383Z",
     "iopub.status.busy": "2025-01-20T03:02:33.238981Z",
     "iopub.status.idle": "2025-01-20T03:02:33.242207Z",
     "shell.execute_reply": "2025-01-20T03:02:33.241570Z"
    },
    "papermill": {
     "duration": 0.009505,
     "end_time": "2025-01-20T03:02:33.243336",
     "exception": false,
     "start_time": "2025-01-20T03:02:33.233831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"ray[train,tune]==2.9.0\" torch==2.1.2 \"ray[default,train]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c4c816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:33.252929Z",
     "iopub.status.busy": "2025-01-20T03:02:33.252702Z",
     "iopub.status.idle": "2025-01-20T03:02:36.632156Z",
     "shell.execute_reply": "2025-01-20T03:02:36.631274Z"
    },
    "papermill": {
     "duration": 3.386293,
     "end_time": "2025-01-20T03:02:36.634093",
     "exception": false,
     "start_time": "2025-01-20T03:02:33.247800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.16.2)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\r\n",
      "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\r\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\r\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.3)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\r\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.560.30)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deepspeed) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.16.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deepspeed) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deepspeed) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad2f270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:36.644922Z",
     "iopub.status.busy": "2025-01-20T03:02:36.644635Z",
     "iopub.status.idle": "2025-01-20T03:02:49.673170Z",
     "shell.execute_reply": "2025-01-20T03:02:49.672153Z"
    },
    "papermill": {
     "duration": 13.035256,
     "end_time": "2025-01-20T03:02:49.674440",
     "exception": false,
     "start_time": "2025-01-20T03:02:36.639184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-20 03:02:36,696] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "PyTorch version: 2.5.1+cu121\n",
      "PyTorch Lightning version: 2.5.0.post0\n",
      "Ray version: 2.40.0\n",
      "DeepSpeed version: 0.16.2\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.2\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing_extensions import Protocol\n",
    "import deepspeed\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620c735b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.685538Z",
     "iopub.status.busy": "2025-01-20T03:02:49.685011Z",
     "iopub.status.idle": "2025-01-20T03:02:49.706539Z",
     "shell.execute_reply": "2025-01-20T03:02:49.705733Z"
    },
    "papermill": {
     "duration": 0.028331,
     "end_time": "2025-01-20T03:02:49.707862",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.679531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data/dataset.py\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Any\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration class for dataset parameters\"\"\"\n",
    "    csv_path: str\n",
    "    batch_size: int = 4\n",
    "    max_length: int = 256\n",
    "    stride: int = 128\n",
    "    tokenizer_name: str = \"gpt2\"\n",
    "    num_workers: int = 4\n",
    "    shuffle: bool = True\n",
    "    drop_last: bool = True\n",
    "    val_split: float = 0.1  # Validation split ratio\n",
    "    seed: int = 42  # Random seed for reproducibility\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found at {self.csv_path}\")\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(\"Batch size must be positive\")\n",
    "        if self.max_length < 1:\n",
    "            raise ValueError(\"Max length must be positive\")\n",
    "        if self.stride < 1:\n",
    "            raise ValueError(\"Stride must be positive\")\n",
    "        if self.num_workers < 0:\n",
    "            raise ValueError(\"Number of workers must be non-negative\")\n",
    "        if not 0 <= self.val_split < 1:\n",
    "            raise ValueError(\"Validation split must be between 0 and 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a94a8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.718261Z",
     "iopub.status.busy": "2025-01-20T03:02:49.718021Z",
     "iopub.status.idle": "2025-01-20T03:02:49.727059Z",
     "shell.execute_reply": "2025-01-20T03:02:49.726439Z"
    },
    "papermill": {
     "duration": 0.01551,
     "end_time": "2025-01-20T03:02:49.728143",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.712633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for processing text data for GPT models\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        texts: List[str],\n",
    "        tokenizer_name: str = \"gpt2\",\n",
    "        max_length: int = 256,\n",
    "        stride: int = 128\n",
    "    ):\n",
    "        \"\"\"Initialize the dataset with texts and processing parameters\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = self._initialize_tokenizer(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Initialize storage for processed data\n",
    "        self.input_ids: List[torch.Tensor] = []\n",
    "        self.target_ids: List[torch.Tensor] = []\n",
    "        \n",
    "        # Process texts\n",
    "        self._process_texts(texts)\n",
    "        \n",
    "        logger.info(f\"Initialized dataset with {len(self)} samples\")\n",
    "    \n",
    "    def _initialize_tokenizer(self, tokenizer_name: str) -> Any:\n",
    "        \"\"\"Initialize the tokenizer with error handling\"\"\"\n",
    "        try:\n",
    "            return tiktoken.get_encoding(tokenizer_name)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize tokenizer: {str(e)}\")\n",
    "            raise RuntimeError(f\"Tokenizer initialization failed: {str(e)}\")\n",
    "    \n",
    "    def _is_valid_text(self, text: Any) -> bool:\n",
    "        \"\"\"Validate text input\"\"\"\n",
    "        if text is None or pd.isna(text):\n",
    "            return False\n",
    "        if isinstance(text, (int, float)):\n",
    "            return not np.isnan(float(text))\n",
    "        return bool(str(text).strip())\n",
    "    \n",
    "    def _process_texts(self, texts: List[str]) -> None:\n",
    "        \"\"\"Process all texts in the dataset\"\"\"\n",
    "        valid_count = 0\n",
    "        for text in texts:\n",
    "            if self._is_valid_text(text):\n",
    "                self._process_single_text(str(text).strip())\n",
    "                valid_count += 1\n",
    "        \n",
    "        if valid_count == 0:\n",
    "            raise ValueError(\"No valid texts found in the dataset\")\n",
    "        \n",
    "        logger.info(f\"Processed {valid_count} valid texts out of {len(texts)} total texts\")\n",
    "    \n",
    "    def _process_single_text(self, text: str) -> None:\n",
    "        \"\"\"Process a single text into chunks\"\"\"\n",
    "        try:\n",
    "            token_ids = self.tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "            \n",
    "            for i in range(0, len(token_ids) - self.max_length, self.stride):\n",
    "                input_chunk = token_ids[i:i + self.max_length]\n",
    "                target_chunk = token_ids[i + 1:i + self.max_length + 1]\n",
    "                \n",
    "                if len(input_chunk) == self.max_length and len(target_chunk) == self.max_length:\n",
    "                    self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "                    self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing text chunk: {str(e)}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a sample from the dataset\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfc315a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.738273Z",
     "iopub.status.busy": "2025-01-20T03:02:49.738059Z",
     "iopub.status.idle": "2025-01-20T03:02:49.747144Z",
     "shell.execute_reply": "2025-01-20T03:02:49.746538Z"
    },
    "papermill": {
     "duration": 0.015365,
     "end_time": "2025-01-20T03:02:49.748231",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.732866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    \"\"\"Manager class for handling dataset creation and dataloader configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        \"\"\"Initialize the dataset manager with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.train_dataset: Optional[GPTDataset] = None\n",
    "        self.val_dataset: Optional[GPTDataset] = None\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        np.random.seed(self.config.seed)\n",
    "    \n",
    "    def _prepare_texts(self, content_series: pd.Series) -> List[str]:\n",
    "        \"\"\"Clean and prepare texts from DataFrame\"\"\"\n",
    "        texts = []\n",
    "        for text in content_series:\n",
    "            if pd.isna(text):\n",
    "                continue\n",
    "            \n",
    "            if isinstance(text, (int, float)):\n",
    "                if not np.isnan(float(text)):\n",
    "                    texts.append(str(text).strip())\n",
    "            elif isinstance(text, str) and text.strip():\n",
    "                texts.append(text.strip())\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def _split_texts(self, texts: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Split texts into training and validation sets\"\"\"\n",
    "        # Shuffle texts deterministically\n",
    "        np.random.shuffle(texts)\n",
    "        \n",
    "        # Calculate split index\n",
    "        val_size = int(len(texts) * self.config.val_split)\n",
    "        \n",
    "        # Split texts\n",
    "        train_texts = texts[val_size:]\n",
    "        val_texts = texts[:val_size]\n",
    "        \n",
    "        logger.info(f\"Split dataset: {len(train_texts)} training samples, {len(val_texts)} validation samples\")\n",
    "        return train_texts, val_texts\n",
    "    \n",
    "    def initialize_datasets(self) -> None:\n",
    "        \"\"\"Initialize training and validation datasets from the CSV file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.csv_path)\n",
    "            texts = self._prepare_texts(df[\"Content\"][:10])\n",
    "            train_texts, val_texts = self._split_texts(texts)\n",
    "            \n",
    "            # Initialize training dataset\n",
    "            self.train_dataset = GPTDataset(\n",
    "                texts=train_texts,\n",
    "                tokenizer_name=self.config.tokenizer_name,\n",
    "                max_length=self.config.max_length,\n",
    "                stride=self.config.stride\n",
    "            )\n",
    "            \n",
    "            # Initialize validation dataset\n",
    "            self.val_dataset = GPTDataset(\n",
    "                texts=val_texts,\n",
    "                tokenizer_name=self.config.tokenizer_name,\n",
    "                max_length=self.config.max_length,\n",
    "                stride=self.config.stride\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Successfully initialized training and validation datasets\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize datasets: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_dataloader(self, dataset: Dataset, is_training: bool = True) -> DataLoader:\n",
    "        \"\"\"Create a DataLoader with the specified configuration\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            num_workers=self.config.num_workers,\n",
    "            shuffle=self.config.shuffle if is_training else False,\n",
    "            drop_last=self.config.drop_last if is_training else False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get the training DataLoader\"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise RuntimeError(\"Datasets not initialized. Call initialize_datasets() first\")\n",
    "        return self._create_dataloader(self.train_dataset, is_training=True)\n",
    "    \n",
    "    def get_val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get the validation DataLoader\"\"\"\n",
    "        if self.val_dataset is None:\n",
    "            raise RuntimeError(\"Datasets not initialized. Call initialize_datasets() first\")\n",
    "        return self._create_dataloader(self.val_dataset, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd74f658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.758431Z",
     "iopub.status.busy": "2025-01-20T03:02:49.758229Z",
     "iopub.status.idle": "2025-01-20T03:02:49.776242Z",
     "shell.execute_reply": "2025-01-20T03:02:49.775596Z"
    },
    "papermill": {
     "duration": 0.024494,
     "end_time": "2025-01-20T03:02:49.777400",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.752906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models/model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for GPT model\"\"\"\n",
    "    vocab_size: int\n",
    "    block_size: int = 1024    # Maximum sequence length\n",
    "    n_layer: int = 12         # Number of transformer layers\n",
    "    n_head: int = 12         # Number of attention heads\n",
    "    n_embd: int = 768        # Embedding dimension\n",
    "    dropout: float = 0.1     # Dropout rate\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self attention\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimension\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        # Decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of GPT model\"\"\"\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # Forward the GPT model\n",
    "        # token embeddings of shape (b, t, n_embd)\n",
    "        token_embeddings = self.tok_emb(idx)\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.dropout(token_embeddings + position_embeddings)\n",
    "        \n",
    "        # Forward through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Final linear layer\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # If we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        do_sample: bool = True,\n",
    "        top_k: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate new tokens from the model.\n",
    "        Args:\n",
    "            idx: (b, t) array of indices in the current sequence\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: temperature for sampling\n",
    "            do_sample: if True, sample from distribution, otherwise take argmax\n",
    "            top_k: if set, only sample from the top k most probable tokens\n",
    "        Returns:\n",
    "            (b, t+n) array of indices in the sequence\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop sequence if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus on last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c646f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.787494Z",
     "iopub.status.busy": "2025-01-20T03:02:49.787295Z",
     "iopub.status.idle": "2025-01-20T03:02:49.794230Z",
     "shell.execute_reply": "2025-01-20T03:02:49.793433Z"
    },
    "papermill": {
     "duration": 0.013221,
     "end_time": "2025-01-20T03:02:49.795363",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.782142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "Number of CPU cores: 4\n",
      "\n",
      "GPU 0: Tesla T4\n",
      "Memory: 14.74 GB\n",
      "Compute Capability: 7.5\n",
      "\n",
      "GPU 1: Tesla T4\n",
      "Memory: 14.74 GB\n",
      "Compute Capability: 7.5\n",
      "CUDA available: True\n",
      "Current GPU name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Check GPU count\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {gpu_count}\")\n",
    "\n",
    "# Check CPU count\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {cpu_count}\")\n",
    "\n",
    "# Check GPU information\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(gpu_count):\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}: {gpu_properties.name}\")\n",
    "        print(f\"Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6024d45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T03:02:49.806191Z",
     "iopub.status.busy": "2025-01-20T03:02:49.805953Z",
     "iopub.status.idle": "2025-01-20T03:11:43.817483Z",
     "shell.execute_reply": "2025-01-20T03:11:43.816142Z"
    },
    "papermill": {
     "duration": 534.019239,
     "end_time": "2025-01-20T03:11:43.819577",
     "exception": false,
     "start_time": "2025-01-20T03:02:49.800338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 03:02:53,281\tINFO worker.py:1821 -- Started a local Ray instance.\n",
      "2025-01-20 03:03:01,487\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:16 (running for 00:00:00.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/4 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(TrainTrainable pid=499)\u001b[0m [2025-01-20 03:03:20,815] [WARNING] [real_accelerator.py:181:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "\u001b[36m(TrainTrainable pid=499)\u001b[0m [2025-01-20 03:03:20,815] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:21 (running for 00:00:05.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=499)\u001b[0m 2025-01-20 03:03:22.746446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(TrainTrainable pid=499)\u001b[0m 2025-01-20 03:03:22.767802: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(TrainTrainable pid=499)\u001b[0m 2025-01-20 03:03:22.774211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:26 (running for 00:00:10.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:31 (running for 00:00:15.17)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:36 (running for 00:00:20.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=499)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=499)\u001b[0m - (node_id=7097748658e24cb109d10619b0730414881dc5dc239aa64622ceb8d7, ip=172.19.2.2, pid=548) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=499)\u001b[0m - (node_id=7097748658e24cb109d10619b0730414881dc5dc239aa64622ceb8d7, ip=172.19.2.2, pid=547) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:03:39,718] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:41 (running for 00:00:25.23)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m 2025-01-20 03:03:43.224879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m 2025-01-20 03:03:43.247534: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m 2025-01-20 03:03:43.254343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:03:45,670] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:03:39,884] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:46 (running for 00:00:30.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m DeepSpeed Config - world_size: 2, micro_batch: 8, train_batch: 16\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:03:48,305] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:03:48,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:03:49,368] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m 2025-01-20 03:03:43.267345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m 2025-01-20 03:03:43.289253: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m 2025-01-20 03:03:43.295792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Creating extension directory /root/.cache/torch_extensions/py310_cu121/fused_adam...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Detected CUDA files, patching ldflags\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Building extension module fused_adam...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:51 (running for 00:00:35.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:03:56 (running for 00:00:40.33)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:01 (running for 00:00:45.37)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:06 (running for 00:00:50.41)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:11 (running for 00:00:55.44)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:16 (running for 00:01:00.47)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:03:45,770] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m DeepSpeed Config - world_size: 2, micro_batch: 8, train_batch: 16\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:03:48,390] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:21 (running for 00:01:05.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Time to load fused_adam op: 34.77862215042114 seconds\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:04:24,170] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,130] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,130] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,135] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,135] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,135] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,136] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Loading extension module fused_adam...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,427] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,428] [INFO] [utils.py:782:see_memory_usage] MA 0.33 GB         Max_MA 0.33 GB         CA 0.34 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,429] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.26 GB, percent = 16.8%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,430] [INFO] [stage3.py:168:__init__] Reduce bucket size 50000000\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,430] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,708] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,709] [INFO] [utils.py:782:see_memory_usage] MA 0.33 GB         Max_MA 0.33 GB         CA 0.34 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,709] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.26 GB, percent = 16.8%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Parameter Offload: Total persistent parameters: 121344 in 98 params\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,212] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,212] [INFO] [utils.py:782:see_memory_usage] MA 0.18 GB         Max_MA 0.36 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,213] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.39 GB, percent = 17.2%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,515] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,515] [INFO] [utils.py:782:see_memory_usage] MA 0.18 GB         Max_MA 0.18 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:25,516] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.39 GB, percent = 17.2%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,352] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,353] [INFO] [utils.py:782:see_memory_usage] MA 0.18 GB         Max_MA 0.18 GB         CA 0.33 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,353] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,647] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,648] [INFO] [utils.py:782:see_memory_usage] MA 0.18 GB         Max_MA 0.18 GB         CA 0.33 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,648] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:26 (running for 00:01:10.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,945] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,946] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.63 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:26,946] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,238] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,239] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,239] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,542] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,543] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.78 GB         CA 1.09 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,543] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,543] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:04:27,618] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,910] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.87 GB         CA 1.09 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.41 GB, percent = 17.3%\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7b0fe08a0b80>\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,912] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"partition_activations\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"contiguous_memory_optimization\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"cpu_checkpointing\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"number_checkpoints\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"synchronize_checkpoint_boundary\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"profile\": false\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"start_step\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"end_step\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"metric_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"arg_mappings\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"metric\": \"throughput\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"model_info\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"results_dir\": \"autotuning_results\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"exps_dir\": \"autotuning_exps\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"overwrite\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"fast\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"start_profile_step\": 3, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"end_profile_step\": 5, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"tuner_type\": \"gridsearch\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"tuner_early_stopping\": 5, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"tuner_num_trials\": 50, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"model_info_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"mp_size\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"max_train_batch_size\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"min_train_batch_size\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"num_tuning_micro_batch_sizes\": 3\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7b0fe1f80130>\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,913] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"recompute_fwd_factor\": 0.0, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"profile_step\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"module_depth\": -1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"top_modules\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"detailed\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"output_file\": null\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,914] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"persistent_storage_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"persistent_time_interval\": 100, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"num_of_version_in_retention\": 2, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"enable_nebula_load\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"load_path\": null\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 0.0001, 'betas': (0.9, 0.95), 'weight_decay': 0.1}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   scheduler_name ............... WarmupLR\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   scheduler_params ............. {'warmup_num_steps': 2000}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   steps_per_print .............. 10\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   train_batch_size ............. 16\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,915] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=50000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,916] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,916] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,916] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,916] [INFO] [config.py:989:print_user_config]   json = {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"train_micro_batch_size_per_gpu\": 8, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"gradient_accumulation_steps\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"train_batch_size\": 16, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"optimizer\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"type\": \"AdamW\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"params\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"lr\": 0.0001, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"betas\": [0.9, 0.95], \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"weight_decay\": 0.1\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"scheduler\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"type\": \"WarmupLR\", \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"params\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"warmup_num_steps\": 2.000000e+03\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         }\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"fp16\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"enabled\": true\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"bf16\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"enabled\": false\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"zero_optimization\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"stage\": 3, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"offload_optimizer\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"device\": \"none\"\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"offload_param\": {\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m             \"device\": \"none\"\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"overlap_comm\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"contiguous_gradients\": true, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"reduce_bucket_size\": 5.000000e+07, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"stage3_prefetch_bucket_size\": 5.000000e+07, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m         \"stage3_param_persistence_threshold\": 1.000000e+05\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"gradient_clipping\": 1.0, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"steps_per_print\": 10, \n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m     \"wall_clock_breakdown\": false\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed366589199644919cd79ce943b26559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308ea5aeddee4d71870df29da0b5578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:29,126] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:29,126] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Time to load fused_adam op: 34.7537956237793 seconds\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:24,712] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:29,951] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:29,952] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,267] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,267] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,580] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,581] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,893] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:30,894] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,207] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,207] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,518] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,518] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:31 (running for 00:01:15.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,833] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:31,833] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,143] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,144] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,457] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,457] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,458] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=10, lr=[0.0], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,458] [INFO] [timer.py:264:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=51.288291640510835, CurrSamplesPerSec=51.32949880907798, MemAllocated=1.12GB, MaxMemAllocated=4.5GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 10, Loss: 11.0234\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,773] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:32,773] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:27,911] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,084] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,084] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,411] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,411] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,727] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:33,727] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,043] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,044] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,364] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,365] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,687] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:34,687] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:35,975] [INFO] [logging.py:128:log_dist] [Rank 0] step=20, skipped=17, lr=[0.00014453708549948897], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:35,976] [INFO] [timer.py:264:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=48.00711928086107, CurrSamplesPerSec=46.04393772159836, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 20, Loss: 11.0078\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:36 (running for 00:01:20.59)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:39,496] [INFO] [logging.py:128:log_dist] [Rank 0] step=30, skipped=17, lr=[0.00033745326572919384], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:39,496] [INFO] [timer.py:264:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=47.124249163064455, CurrSamplesPerSec=45.37788514411522, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 30, Loss: 4.1523\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:41 (running for 00:01:25.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:43,004] [INFO] [logging.py:128:log_dist] [Rank 0] step=40, skipped=17, lr=[0.00041251604432745857], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:43,005] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=46.754821703545744, CurrSamplesPerSec=45.78768310321067, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 40, Loss: 2.5820\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:46,527] [INFO] [logging.py:128:log_dist] [Rank 0] step=50, skipped=17, lr=[0.0004600121604082693], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:46,528] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=46.49653731858183, CurrSamplesPerSec=45.33294729414702, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 50, Loss: 3.6934\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:46 (running for 00:01:30.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:50,061] [INFO] [logging.py:128:log_dist] [Rank 0] step=60, skipped=17, lr=[0.0004948359929249976], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:50,061] [INFO] [timer.py:264:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=46.31238936105126, CurrSamplesPerSec=45.34495469771469, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 60, Loss: 4.4375\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:51 (running for 00:01:35.68)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:53,629] [INFO] [logging.py:128:log_dist] [Rank 0] step=70, skipped=17, lr=[0.0005223448050655965], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:53,630] [INFO] [timer.py:264:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=46.11113668460743, CurrSamplesPerSec=43.176701047295275, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 70, Loss: 2.1484\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:04:56 (running for 00:01:40.72)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:57,299] [INFO] [logging.py:128:log_dist] [Rank 0] step=80, skipped=17, lr=[0.0005450845802119577], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:04:57,299] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=45.79389965988492, CurrSamplesPerSec=44.70907315387769, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 80, Loss: 4.2969\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:00,907] [INFO] [logging.py:128:log_dist] [Rank 0] step=90, skipped=17, lr=[0.0005644671095288428], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:00,908] [INFO] [timer.py:264:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=45.643543617926504, CurrSamplesPerSec=44.14980758323594, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 90, Loss: 3.3438\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:01 (running for 00:01:45.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:04,520] [INFO] [logging.py:128:log_dist] [Rank 0] step=100, skipped=17, lr=[0.000581357362670697], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:04,520] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=45.515721681445015, CurrSamplesPerSec=45.030645148548324, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 100, Loss: 3.8965\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:06 (running for 00:01:50.78)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:08,123] [INFO] [logging.py:128:log_dist] [Rank 0] step=110, skipped=17, lr=[0.0005963238598678615], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:08,123] [INFO] [timer.py:264:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=45.42483208654352, CurrSamplesPerSec=44.727011489767975, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 110, Loss: 3.5527\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:11,741] [INFO] [logging.py:128:log_dist] [Rank 0] step=120, skipped=17, lr=[0.0006097603558129145], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:11,742] [INFO] [timer.py:264:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=45.332906531277075, CurrSamplesPerSec=44.42497464141975, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 120, Loss: 2.8008\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:11 (running for 00:01:55.80)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:15,372] [INFO] [logging.py:128:log_dist] [Rank 0] step=130, skipped=17, lr=[0.000621950859634784], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:15,373] [INFO] [timer.py:264:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=45.24071073286944, CurrSamplesPerSec=43.0415906719482, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 130, Loss: 2.2559\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:16 (running for 00:02:00.83)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:19,007] [INFO] [logging.py:128:log_dist] [Rank 0] step=140, skipped=17, lr=[0.0006331069739398193], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:19,008] [INFO] [timer.py:264:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=45.158504255396934, CurrSamplesPerSec=43.968239240042934, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 140, Loss: 4.5938\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:22 (running for 00:02:05.86)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:22,671] [INFO] [logging.py:128:log_dist] [Rank 0] step=150, skipped=17, lr=[0.0006433905913478035], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:22,671] [INFO] [timer.py:264:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=45.068710163880226, CurrSamplesPerSec=43.6829824915033, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 150, Loss: 7.0234\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:26,434] [INFO] [logging.py:128:log_dist] [Rank 0] step=160, skipped=17, lr=[0.000652928340637974], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:26,434] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=44.909247475693135, CurrSamplesPerSec=40.712672643156445, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 160, Loss: 5.9609\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:27 (running for 00:02:10.89)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:30,155] [INFO] [logging.py:128:log_dist] [Rank 0] step=170, skipped=17, lr=[0.0006618211387619221], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:30,156] [INFO] [timer.py:264:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=44.797291779529516, CurrSamplesPerSec=42.8557278751924, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 170, Loss: 6.1953\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:32 (running for 00:02:15.92)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:33,868] [INFO] [logging.py:128:log_dist] [Rank 0] step=180, skipped=17, lr=[0.0006701507127501854], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:33,869] [INFO] [timer.py:264:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=44.70751255615411, CurrSamplesPerSec=43.38740132101042, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 180, Loss: 5.5156\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:37 (running for 00:02:20.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:37,611] [INFO] [logging.py:128:log_dist] [Rank 0] step=190, skipped=17, lr=[0.0006779841764747815], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:37,612] [INFO] [timer.py:264:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=44.60898033121481, CurrSamplesPerSec=42.804747882603074, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 190, Loss: 4.9961\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:41,380] [INFO] [logging.py:128:log_dist] [Rank 0] step=200, skipped=17, lr=[0.0006853773194131041], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:41,380] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=44.50202559319221, CurrSamplesPerSec=42.91711927979872, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 200, Loss: 1.8877\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:42 (running for 00:02:25.98)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:45,141] [INFO] [logging.py:128:log_dist] [Rank 0] step=210, skipped=17, lr=[0.0006923770192969873], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:45,142] [INFO] [timer.py:264:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=44.412018897126636, CurrSamplesPerSec=41.187855664537516, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 210, Loss: 1.0742\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:47 (running for 00:02:31.01)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:48,911] [INFO] [logging.py:128:log_dist] [Rank 0] step=220, skipped=17, lr=[0.0006990230446085586], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:48,912] [INFO] [timer.py:264:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=44.32311167630424, CurrSamplesPerSec=42.659121497717344, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 220, Loss: 2.0547\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:52 (running for 00:02:36.03)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:52,699] [INFO] [logging.py:128:log_dist] [Rank 0] step=230, skipped=17, lr=[0.0007053494232094669], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:52,699] [INFO] [timer.py:264:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=44.23492583887881, CurrSamplesPerSec=42.28957147158189, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 230, Loss: 1.2383\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:56,556] [INFO] [logging.py:128:log_dist] [Rank 0] step=240, skipped=17, lr=[0.0007113854966882281], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:05:56,557] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=44.11632509927058, CurrSamplesPerSec=41.21130256126015, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 240, Loss: 5.3945\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:05:57 (running for 00:02:41.07)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:00,465] [INFO] [logging.py:128:log_dist] [Rank 0] step=250, skipped=17, lr=[0.0007171567432394205], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:00,465] [INFO] [timer.py:264:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=43.98512386643482, CurrSamplesPerSec=41.58857036568598, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 250, Loss: 3.7656\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:02 (running for 00:02:46.09)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:04,359] [INFO] [logging.py:128:log_dist] [Rank 0] step=260, skipped=17, lr=[0.0007226854274974447], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:04,359] [INFO] [timer.py:264:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=43.869356627316066, CurrSamplesPerSec=40.684781895166026, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 260, Loss: 3.4551\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:07 (running for 00:02:51.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:08,307] [INFO] [logging.py:128:log_dist] [Rank 0] step=270, skipped=17, lr=[0.0007279911192362388], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:08,307] [INFO] [timer.py:264:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=43.73975562967173, CurrSamplesPerSec=40.71126484893831, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 270, Loss: 5.1133\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:12,274] [INFO] [logging.py:128:log_dist] [Rank 0] step=280, skipped=17, lr=[0.000733091111461712], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:12,275] [INFO] [timer.py:264:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=43.61334655451715, CurrSamplesPerSec=40.50825790339993, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 280, Loss: 5.1562\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:12 (running for 00:02:56.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:16,298] [INFO] [logging.py:128:log_dist] [Rank 0] step=290, skipped=17, lr=[0.0007380007604416626], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:16,299] [INFO] [timer.py:264:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=43.470909548513156, CurrSamplesPerSec=39.870089528372425, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 290, Loss: 5.2617\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:17 (running for 00:03:01.19)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:20,329] [INFO] [logging.py:128:log_dist] [Rank 0] step=300, skipped=17, lr=[0.0007427337645355536], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:20,330] [INFO] [timer.py:264:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=43.3374396633795, CurrSamplesPerSec=39.904869027813966, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 300, Loss: 5.8984\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:22 (running for 00:03:06.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:24,381] [INFO] [logging.py:128:log_dist] [Rank 0] step=310, skipped=17, lr=[0.0007473023945842441], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:24,381] [INFO] [timer.py:264:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=43.205577824341546, CurrSamplesPerSec=39.37557543192603, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 310, Loss: 6.2070\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:27 (running for 00:03:11.24)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:28,438] [INFO] [logging.py:128:log_dist] [Rank 0] step=320, skipped=17, lr=[0.0007517176856198723], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:28,439] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=43.078566632334, CurrSamplesPerSec=38.117968684641404, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 320, Loss: 5.0938\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:32 (running for 00:03:16.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:32,447] [INFO] [logging.py:128:log_dist] [Rank 0] step=330, skipped=17, lr=[0.0007559895974360832], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:32,448] [INFO] [timer.py:264:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=42.97912896810615, CurrSamplesPerSec=50.04952405168463, MemAllocated=1.64GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 0, Batch: 330, Loss: 6.0117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115668c68e914771adc0b174f6229024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a45a03098b41a886cead533ff7a99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Loading extension module fused_adam...\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:37 (running for 00:03:21.32)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:42 (running for 00:03:26.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch 0: {'train_loss': 4.706414690884677, 'val_loss': 6.997663225446429, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:06:45,264] [INFO] [logging.py:128:log_dist] [Rank 1] Saving model checkpoint: /tmp/tmp8ygancw3/global_step330/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:06:45,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmp8ygancw3/global_step330/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:45,259] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step330 is about to be saved!\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:06:45,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmp8ygancw3/global_step330/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:06:47,382] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmp8ygancw3/global_step330/zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:06:47,394] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step330 is ready now!\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:47 (running for 00:03:31.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000000)\n",
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e28354604c4ea6a90786c6d156b76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae1d43eca9c4e2886f17956b7bed239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:45,264] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/tmpe9502apj/global_step330/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:45,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmpe9502apj/global_step330/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:47,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmpe9502apj/global_step330/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:52 (running for 00:03:36.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:52,706] [INFO] [logging.py:128:log_dist] [Rank 0] step=340, skipped=17, lr=[0.0007601271498977679], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:52,707] [INFO] [timer.py:264:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=42.910033067544816, CurrSamplesPerSec=41.42059186708974, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 10, Loss: 4.6406\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:47,386] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmpe9502apj/global_step330/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:47,395] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step330 is ready now!\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:56,751] [INFO] [logging.py:128:log_dist] [Rank 0] step=350, skipped=17, lr=[0.0007641385376139082], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:06:56,752] [INFO] [timer.py:264:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=42.81254722540684, CurrSamplesPerSec=41.479681001124355, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 20, Loss: 5.5469\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:06:57 (running for 00:03:41.44)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:00,643] [INFO] [logging.py:128:log_dist] [Rank 0] step=360, skipped=17, lr=[0.0007680312276389396], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:00,644] [INFO] [timer.py:264:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=42.76632840614247, CurrSamplesPerSec=40.00172636082075, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 30, Loss: 2.9316\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:02 (running for 00:03:46.47)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:04,504] [INFO] [logging.py:128:log_dist] [Rank 0] step=370, skipped=17, lr=[0.0007718120431303001], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:04,505] [INFO] [timer.py:264:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=42.73204164456798, CurrSamplesPerSec=41.77494299995486, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 40, Loss: 2.2930\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:07 (running for 00:03:51.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:08,301] [INFO] [logging.py:128:log_dist] [Rank 0] step=380, skipped=17, lr=[0.0007754872353170495], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:08,301] [INFO] [timer.py:264:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=42.719478170365946, CurrSamplesPerSec=41.64485776107915, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 50, Loss: 3.0234\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:12,180] [INFO] [logging.py:128:log_dist] [Rank 0] step=390, skipped=17, lr=[0.0007790625456862609], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:12,181] [INFO] [timer.py:264:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=42.68288440945401, CurrSamplesPerSec=41.251302216400596, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 60, Loss: 3.3828\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:12 (running for 00:03:56.53)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:16,066] [INFO] [logging.py:128:log_dist] [Rank 0] step=400, skipped=17, lr=[0.000782543259940608], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:16,067] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=42.64667688436078, CurrSamplesPerSec=41.47152959194305, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 70, Loss: 1.7002\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:17 (running for 00:04:01.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:19,984] [INFO] [logging.py:128:log_dist] [Rank 0] step=410, skipped=17, lr=[0.0007859342550001826], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:19,985] [INFO] [timer.py:264:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=42.604080487979154, CurrSamplesPerSec=40.92128177617078, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 80, Loss: 2.8184\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:22 (running for 00:04:06.59)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:23,921] [INFO] [logging.py:128:log_dist] [Rank 0] step=420, skipped=17, lr=[0.0007892400400975663], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:23,922] [INFO] [timer.py:264:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=42.55775986079113, CurrSamplesPerSec=41.89841310190319, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 90, Loss: 2.4727\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:27 (running for 00:04:11.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:27,867] [INFO] [logging.py:128:log_dist] [Rank 0] step=430, skipped=17, lr=[0.0007924647928351284], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:27,868] [INFO] [timer.py:264:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=42.51224166806003, CurrSamplesPerSec=40.57671561579094, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 100, Loss: 3.1309\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:31,772] [INFO] [logging.py:128:log_dist] [Rank 0] step=440, skipped=17, lr=[0.0007956123909279325], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:31,772] [INFO] [timer.py:264:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=42.47891803682829, CurrSamplesPerSec=41.03990070130634, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 110, Loss: 2.4434\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:32 (running for 00:04:16.64)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:35,742] [INFO] [logging.py:128:log_dist] [Rank 0] step=450, skipped=17, lr=[0.000798686440237285], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:35,743] [INFO] [timer.py:264:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=42.4306235069324, CurrSamplesPerSec=40.64486352610373, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 120, Loss: 1.9541\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:37 (running for 00:04:21.67)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:39,658] [INFO] [logging.py:128:log_dist] [Rank 0] step=460, skipped=17, lr=[0.0008016902996032191], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:39,658] [INFO] [timer.py:264:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=42.39784067650544, CurrSamplesPerSec=41.08298832712132, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 130, Loss: 1.4092\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:42 (running for 00:04:26.70)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:43,552] [INFO] [logging.py:128:log_dist] [Rank 0] step=470, skipped=17, lr=[0.0008046271029047633], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:43,552] [INFO] [timer.py:264:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=42.37205604893644, CurrSamplesPerSec=40.67373489978103, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 140, Loss: 3.0410\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:47,469] [INFO] [logging.py:128:log_dist] [Rank 0] step=480, skipped=17, lr=[0.0008074997787112774], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:47,470] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=42.340968650189254, CurrSamplesPerSec=40.55165703303215, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 150, Loss: 5.9023\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:47 (running for 00:04:31.73)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:51,386] [INFO] [logging.py:128:log_dist] [Rank 0] step=490, skipped=17, lr=[0.0008103110678337778], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:51,386] [INFO] [timer.py:264:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=42.31167636185052, CurrSamplesPerSec=40.8006673599233, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 160, Loss: 4.7266\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:52 (running for 00:04:36.76)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:55,299] [INFO] [logging.py:128:log_dist] [Rank 0] step=500, skipped=17, lr=[0.0008130635390399272], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:55,300] [INFO] [timer.py:264:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=42.284133335593026, CurrSamplesPerSec=41.08859762263795, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 170, Loss: 5.1641\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:07:57 (running for 00:04:41.79)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:59,180] [INFO] [logging.py:128:log_dist] [Rank 0] step=510, skipped=17, lr=[0.000815759603158523], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:07:59,181] [INFO] [timer.py:264:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=42.265855297897055, CurrSamplesPerSec=41.528605429953615, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 180, Loss: 4.6055\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:02 (running for 00:04:46.82)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:03,054] [INFO] [logging.py:128:log_dist] [Rank 0] step=520, skipped=17, lr=[0.0008184015257675732], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:03,055] [INFO] [timer.py:264:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=42.24900045085673, CurrSamplesPerSec=41.04760712059721, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 190, Loss: 3.9648\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:06,988] [INFO] [logging.py:128:log_dist] [Rank 0] step=530, skipped=17, lr=[0.0008209914386332905], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:06,989] [INFO] [timer.py:264:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=42.22102134537365, CurrSamplesPerSec=42.30815423312966, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 200, Loss: 1.6084\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:08 (running for 00:04:51.85)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:10,834] [INFO] [logging.py:128:log_dist] [Rank 0] step=540, skipped=17, lr=[0.0008235313500447201], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:10,835] [INFO] [timer.py:264:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=42.21171807593107, CurrSamplesPerSec=41.583313300502276, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 210, Loss: 0.9404\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:13 (running for 00:04:56.88)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:14,690] [INFO] [logging.py:128:log_dist] [Rank 0] step=550, skipped=17, lr=[0.0008260231541695242], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:14,691] [INFO] [timer.py:264:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=42.20024636241241, CurrSamplesPerSec=41.137687635607875, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 220, Loss: 1.7490\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:18 (running for 00:05:01.91)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:18,555] [INFO] [logging.py:128:log_dist] [Rank 0] step=560, skipped=17, lr=[0.0008284686395401141], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:18,556] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=42.187934866195306, CurrSamplesPerSec=41.85153699479448, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 230, Loss: 1.0908\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:22,450] [INFO] [logging.py:128:log_dist] [Rank 0] step=570, skipped=17, lr=[0.0008308694967653625], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:22,451] [INFO] [timer.py:264:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=42.17059303975051, CurrSamplesPerSec=41.73581660217265, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 240, Loss: 4.3281\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:23 (running for 00:05:06.94)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:26,343] [INFO] [logging.py:128:log_dist] [Rank 0] step=580, skipped=17, lr=[0.0008332273255511874], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:26,344] [INFO] [timer.py:264:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=42.15322932013327, CurrSamplesPerSec=41.46853129472171, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 250, Loss: 3.0625\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:28 (running for 00:05:11.97)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:30,245] [INFO] [logging.py:128:log_dist] [Rank 0] step=590, skipped=17, lr=[0.0008355436411030265], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:30,246] [INFO] [timer.py:264:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=42.13494262881907, CurrSamplesPerSec=41.3148682330318, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 260, Loss: 2.7910\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:33 (running for 00:05:17.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:34,159] [INFO] [logging.py:128:log_dist] [Rank 0] step=600, skipped=17, lr=[0.0008378198799743769], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:34,160] [INFO] [timer.py:264:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=42.115300072544315, CurrSamplesPerSec=40.50627742080183, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 270, Loss: 4.1836\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:38 (running for 00:05:22.03)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:38,134] [INFO] [logging.py:128:log_dist] [Rank 0] step=610, skipped=17, lr=[0.0008400574054179355], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:38,135] [INFO] [timer.py:264:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=42.084784999493515, CurrSamplesPerSec=40.80481036260543, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 280, Loss: 4.3711\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:42,080] [INFO] [logging.py:128:log_dist] [Rank 0] step=620, skipped=17, lr=[0.0008422575122892541], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:42,080] [INFO] [timer.py:264:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=42.06096639272913, CurrSamplesPerSec=40.89988350433996, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 290, Loss: 4.3398\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:43 (running for 00:05:27.06)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:46,018] [INFO] [logging.py:128:log_dist] [Rank 0] step=630, skipped=17, lr=[0.0008444214315470756], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:46,019] [INFO] [timer.py:264:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=42.03964281489382, CurrSamplesPerSec=40.90641533472385, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 300, Loss: 5.0547\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:48 (running for 00:05:32.09)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:49,929] [INFO] [logging.py:128:log_dist] [Rank 0] step=640, skipped=17, lr=[0.0008465503343895172], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:49,930] [INFO] [timer.py:264:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=42.02342585713995, CurrSamplesPerSec=41.0790149549403, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 310, Loss: 6.0586\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:53 (running for 00:05:37.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:53,850] [INFO] [logging.py:128:log_dist] [Rank 0] step=650, skipped=17, lr=[0.0008486453360608953], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:53,850] [INFO] [timer.py:264:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=42.00577035920721, CurrSamplesPerSec=40.373320399915265, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 320, Loss: 4.6484\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:57,729] [INFO] [logging.py:128:log_dist] [Rank 0] step=660, skipped=17, lr=[0.0008507074993601712], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:08:57,730] [INFO] [timer.py:264:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=41.99514776922373, CurrSamplesPerSec=49.91105276269408, MemAllocated=1.64GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 1, Batch: 330, Loss: 5.0703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0041a0353e34ec5be810f1b3d99d25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1dfc0c051442dc9f3edd28ffcd6bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000000)\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:08:58 (running for 00:05:42.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:03 (running for 00:05:47.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:08 (running for 00:05:52.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch 1: {'train_loss': 3.4944349057746655, 'val_loss': 6.442034040178571, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:10,619] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step660 is about to be saved!\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:10,623] [INFO] [logging.py:128:log_dist] [Rank 1] Saving model checkpoint: /tmp/tmp4whrzun6/global_step660/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:10,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmp4whrzun6/global_step660/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:10,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmp4whrzun6/global_step660/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:10,667] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmp4whrzun6/global_step660/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:12,789] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmp4whrzun6/global_step660/zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:09:12,799] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step660 is ready now!\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:13 (running for 00:05:57.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000001)\n",
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=635)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe885e8b6a534ac3bce829f8fbcf1841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d056c4b14d964d22a7e2dd9d6f600af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=635) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:10,624] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/tmpseo7h9d6/global_step660/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:10,667] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmpseo7h9d6/global_step660/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:12,785] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmpseo7h9d6/global_step660/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:12,786] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmpseo7h9d6/global_step660/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:12,798] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step660 is ready now!\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:18 (running for 00:06:02.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:18,770] [INFO] [logging.py:128:log_dist] [Rank 0] step=670, skipped=17, lr=[0.0008527378378786505], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:18,771] [INFO] [timer.py:264:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=41.97520283152515, CurrSamplesPerSec=41.387945028617274, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 10, Loss: 3.8965\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:22,753] [INFO] [logging.py:128:log_dist] [Rank 0] step=680, skipped=17, lr=[0.000854737318991627], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:22,754] [INFO] [timer.py:264:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=41.94882258022769, CurrSamplesPerSec=41.96354458636373, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 20, Loss: 4.9570\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:23 (running for 00:06:07.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:26,648] [INFO] [logging.py:128:log_dist] [Rank 0] step=690, skipped=17, lr=[0.0008567068666260755], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:26,648] [INFO] [timer.py:264:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=41.93765031856085, CurrSamplesPerSec=40.956320472969715, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 30, Loss: 2.5391\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:28 (running for 00:06:12.32)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:30,508] [INFO] [logging.py:128:log_dist] [Rank 0] step=700, skipped=17, lr=[0.0008586473638242137], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:30,508] [INFO] [timer.py:264:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=41.93254143280572, CurrSamplesPerSec=41.8709645175143, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 40, Loss: 1.9795\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:33 (running for 00:06:17.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:34,337] [INFO] [logging.py:128:log_dist] [Rank 0] step=710, skipped=17, lr=[0.000860559655120738], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:34,338] [INFO] [timer.py:264:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=41.932164252077385, CurrSamplesPerSec=41.419134690148276, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 50, Loss: 2.6602\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:38,209] [INFO] [logging.py:128:log_dist] [Rank 0] step=720, skipped=17, lr=[0.0008624445487497539], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:38,210] [INFO] [timer.py:264:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=41.925906325842334, CurrSamplesPerSec=40.97865363360251, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 60, Loss: 2.9199\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:38 (running for 00:06:22.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:42,126] [INFO] [logging.py:128:log_dist] [Rank 0] step=730, skipped=17, lr=[0.000864302818695831], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:42,127] [INFO] [timer.py:264:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=41.91247699574129, CurrSamplesPerSec=42.01259199561357, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 70, Loss: 1.4287\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:43 (running for 00:06:27.42)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:45,995] [INFO] [logging.py:128:log_dist] [Rank 0] step=740, skipped=17, lr=[0.000866135206602215], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:45,996] [INFO] [timer.py:264:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=41.90618558946789, CurrSamplesPerSec=41.363429915139186, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 80, Loss: 2.2168\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:48 (running for 00:06:32.45)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:49,874] [INFO] [logging.py:128:log_dist] [Rank 0] step=750, skipped=17, lr=[0.0008679424235479662], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:49,875] [INFO] [timer.py:264:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=41.898806686189594, CurrSamplesPerSec=41.48660452471144, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 90, Loss: 2.0664\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:53 (running for 00:06:37.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:53,746] [INFO] [logging.py:128:log_dist] [Rank 0] step=760, skipped=17, lr=[0.0008697251517046863], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:53,746] [INFO] [timer.py:264:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=41.892327348135865, CurrSamplesPerSec=41.68383961272266, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 100, Loss: 2.7480\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:57,623] [INFO] [logging.py:128:log_dist] [Rank 0] step=770, skipped=17, lr=[0.0008714840458824888], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:09:57,624] [INFO] [timer.py:264:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=41.88559578695769, CurrSamplesPerSec=41.14691926579855, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 110, Loss: 2.0078\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:09:58 (running for 00:06:42.51)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:01,508] [INFO] [logging.py:128:log_dist] [Rank 0] step=780, skipped=17, lr=[0.0008732197349739864], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:01,509] [INFO] [timer.py:264:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=41.877658640016584, CurrSamplesPerSec=41.34757816864886, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 120, Loss: 1.6123\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:03 (running for 00:06:47.53)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:05,395] [INFO] [logging.py:128:log_dist] [Rank 0] step=790, skipped=17, lr=[0.0008749328233042567], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:05,395] [INFO] [timer.py:264:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=41.86974768413993, CurrSamplesPerSec=41.35374412423102, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 130, Loss: 1.1318\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:08 (running for 00:06:52.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:09,279] [INFO] [logging.py:128:log_dist] [Rank 0] step=800, skipped=17, lr=[0.0008766238918940456], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:09,280] [INFO] [timer.py:264:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=41.86241102259453, CurrSamplesPerSec=40.89868705920661, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 140, Loss: 2.4199\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:13,251] [INFO] [logging.py:128:log_dist] [Rank 0] step=810, skipped=17, lr=[0.0008782934996428088], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:13,251] [INFO] [timer.py:264:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=41.84367602864937, CurrSamplesPerSec=40.8656878015477, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 150, Loss: 5.4141\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:13 (running for 00:06:57.59)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:17,167] [INFO] [logging.py:128:log_dist] [Rank 0] step=820, skipped=17, lr=[0.0008799421844376231], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:17,168] [INFO] [timer.py:264:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=41.83313896138381, CurrSamplesPerSec=41.13360282661284, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 160, Loss: 4.1094\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:18 (running for 00:07:02.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:21,104] [INFO] [logging.py:128:log_dist] [Rank 0] step=830, skipped=17, lr=[0.0008815704641934713], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:21,105] [INFO] [timer.py:264:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=41.81980963513262, CurrSamplesPerSec=40.47429867518859, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 170, Loss: 4.7578\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:23 (running for 00:07:07.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:25,029] [INFO] [logging.py:128:log_dist] [Rank 0] step=840, skipped=17, lr=[0.0008831788378299351], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:25,030] [INFO] [timer.py:264:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=41.8083326355084, CurrSamplesPerSec=40.97695215550276, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 180, Loss: 4.1680\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:28 (running for 00:07:12.68)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:28,961] [INFO] [logging.py:128:log_dist] [Rank 0] step=850, skipped=17, lr=[0.0008847677861889041], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:28,962] [INFO] [timer.py:264:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=41.796270308231335, CurrSamplesPerSec=41.02770687716571, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 190, Loss: 3.5781\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:32,849] [INFO] [logging.py:128:log_dist] [Rank 0] step=860, skipped=17, lr=[0.0008863377728975259], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:32,850] [INFO] [timer.py:264:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=41.789823493421494, CurrSamplesPerSec=41.7092545630025, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 200, Loss: 1.4482\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:33 (running for 00:07:17.71)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:36,712] [INFO] [logging.py:128:log_dist] [Rank 0] step=870, skipped=17, lr=[0.0008878892451802704], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:36,712] [INFO] [timer.py:264:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=41.78698990426277, CurrSamplesPerSec=41.64165347600502, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 210, Loss: 0.8350\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:38 (running for 00:07:22.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:40,565] [INFO] [logging.py:128:log_dist] [Rank 0] step=880, skipped=17, lr=[0.0008894226346236669], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:40,566] [INFO] [timer.py:264:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=41.785677719568035, CurrSamplesPerSec=41.74908434005394, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 220, Loss: 1.6025\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:43 (running for 00:07:27.77)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:44,469] [INFO] [logging.py:128:log_dist] [Rank 0] step=890, skipped=17, lr=[0.0008909383578969884], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:44,470] [INFO] [timer.py:264:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=41.77778120578431, CurrSamplesPerSec=41.47252912070171, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 230, Loss: 0.9775\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:48,330] [INFO] [logging.py:128:log_dist] [Rank 0] step=900, skipped=17, lr=[0.00089243681743189], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:48,331] [INFO] [timer.py:264:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=41.77541944406713, CurrSamplesPerSec=42.15483658779746, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 240, Loss: 3.9141\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:48 (running for 00:07:32.80)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:52,190] [INFO] [logging.py:128:log_dist] [Rank 0] step=910, skipped=17, lr=[0.0008939184020637782], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:52,190] [INFO] [timer.py:264:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=41.773381872101396, CurrSamplesPerSec=41.505668972751195, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 250, Loss: 2.7617\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:53 (running for 00:07:37.83)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:56,056] [INFO] [logging.py:128:log_dist] [Rank 0] step=920, skipped=17, lr=[0.0008953834876374665], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:56,056] [INFO] [timer.py:264:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=41.77056878683408, CurrSamplesPerSec=41.330262181189106, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 260, Loss: 2.4746\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:10:59 (running for 00:07:42.86)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:59,928] [INFO] [logging.py:128:log_dist] [Rank 0] step=930, skipped=17, lr=[0.0008968324375794772], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:10:59,929] [INFO] [timer.py:264:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=41.76680386279539, CurrSamplesPerSec=41.84822253876247, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 270, Loss: 3.7598\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:03,804] [INFO] [logging.py:128:log_dist] [Rank 0] step=940, skipped=17, lr=[0.0008982656034391717], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:03,805] [INFO] [timer.py:264:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=41.762898098788185, CurrSamplesPerSec=41.20624164146362, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 280, Loss: 3.9570\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:04 (running for 00:07:47.89)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:07,715] [INFO] [logging.py:128:log_dist] [Rank 0] step=950, skipped=17, lr=[0.0008996833254007216], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:07,716] [INFO] [timer.py:264:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=41.754941202300955, CurrSamplesPerSec=41.022490335108934, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 290, Loss: 4.0547\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:09 (running for 00:07:52.92)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:11,601] [INFO] [logging.py:128:log_dist] [Rank 0] step=960, skipped=17, lr=[0.0009010859327677889], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:11,602] [INFO] [timer.py:264:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=41.7499620870308, CurrSamplesPerSec=41.282311589683204, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 300, Loss: 4.6914\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:14 (running for 00:07:57.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:15,562] [INFO] [logging.py:128:log_dist] [Rank 0] step=970, skipped=17, lr=[0.0009024737444226407], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:15,562] [INFO] [timer.py:264:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=41.73715158519149, CurrSamplesPerSec=40.200229541732696, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 310, Loss: 6.2383\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:19 (running for 00:08:02.98)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:19,472] [INFO] [logging.py:128:log_dist] [Rank 0] step=980, skipped=17, lr=[0.0009038470692613011], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:19,473] [INFO] [timer.py:264:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=41.729564980729116, CurrSamplesPerSec=40.51018967175268, MemAllocated=1.73GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 320, Loss: 4.2148\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:23,315] [INFO] [logging.py:128:log_dist] [Rank 0] step=990, skipped=17, lr=[0.000905206206606224], mom=[(0.9, 0.95)]\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:23,316] [INFO] [timer.py:264:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=41.72960828513972, CurrSamplesPerSec=50.47258533929106, MemAllocated=1.64GB, MaxMemAllocated=5.11GB\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch: 2, Batch: 330, Loss: 4.6562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6537175cc226422f9f84225f776a3bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be958a6acdf44c05b6abde541e10172c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=636) - split(2, equal=True) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000001)\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-20_03-02-49_860434_18/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=636)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> OutputSplitter[split(2, equal=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:24 (running for 00:08:08.01)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:29 (running for 00:08:13.04)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:34 (running for 00:08:18.07)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Epoch 2: {'train_loss': 3.104019598527388, 'val_loss': 6.307198660714286, 'epoch': 2}\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:36,280] [INFO] [logging.py:128:log_dist] [Rank 1] Saving model checkpoint: /tmp/tmpzhgfrh1o/global_step990/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:36,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmpzhgfrh1o/global_step990/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:36,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmpzhgfrh1o/global_step990/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:36,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmpzhgfrh1o/global_step990/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:36,276] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step990 is about to be saved!\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:38,373] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmpzhgfrh1o/global_step990/zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m [2025-01-20 03:11:38,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step990 is ready now!\n",
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:39 (running for 00:08:23.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000002)\n",
      "2025-01-20 03:11:41,990\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2025-01-20_03-03-01' in 0.0086s.\n",
      "2025-01-20 03:11:41,994\tINFO tune.py:1041 -- Total run time: 520.51 seconds (505.82 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=547)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2025-01-20_03-03-01/TorchTrainer_0f514_00000_0_2025-01-20_03-03-16/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-20 03:11:41 (running for 00:08:25.83)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 4.0/4 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /tmp/ray/session_2025-01-20_03-02-49_860434_18/artifacts/2025-01-20_03-03-01/TorchTrainer_2025-01-20_03-03-01/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:36,281] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/tmplea7_9op/global_step990/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:36,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tmplea7_9op/global_step990/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:38,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tmplea7_9op/global_step990/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:38,383] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved /tmp/tmplea7_9op/global_step990/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=548)\u001b[0m [2025-01-20 03:11:38,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step990 is ready now!\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import ray\n",
    "import ray.train\n",
    "from ray.train import Checkpoint, DataConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import deepspeed\n",
    "from deepspeed.accelerator import get_accelerator\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class RayConfig:\n",
    "    \"\"\"Ray-specific configuration for distributed training\"\"\"\n",
    "    num_workers: int = 2         # Use both GPUs\n",
    "    cpus_per_worker: int = 2     # 4 CPUs total, so 2 per worker\n",
    "    gpus_per_worker: float = 1.0 # Each worker gets 1 full GPU\n",
    "    use_gpu: bool = True\n",
    "    address: Optional[str] = None\n",
    "\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Auto-detect GPU configuration\"\"\"\n",
    "        if self.num_workers is None:\n",
    "            if torch.cuda.is_available():\n",
    "                # Use all available GPUs\n",
    "                self.num_workers = torch.cuda.device_count()\n",
    "            else:\n",
    "                self.num_workers = 1\n",
    "                self.use_gpu = False\n",
    "                self.gpus_per_worker = 0\n",
    "    \n",
    "    def get_scaling_config(self) -> ScalingConfig:\n",
    "        \"\"\"Get Ray scaling configuration for multi-GPU\"\"\"\n",
    "        return ScalingConfig(\n",
    "            num_workers=self.num_workers,\n",
    "            use_gpu=self.use_gpu,\n",
    "            resources_per_worker={\n",
    "                \"CPU\": self.cpus_per_worker,\n",
    "                \"GPU\": self.gpus_per_worker\n",
    "            },\n",
    "            placement_strategy=\"PACK\" if self.use_gpu else \"SPREAD\"\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training parameters\"\"\"\n",
    "    # Model configuration\n",
    "    vocab_size: int = 50257  # GPT-2 vocabulary size\n",
    "    block_size: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training configuration\n",
    "    num_epochs: int = 3\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 32\n",
    "    log_interval: int = 10\n",
    "    data_path: str = \"/kaggle/input/llms-txt/files.csv\"\n",
    "    tokenizer_name: str = \"gpt2\"\n",
    "    \n",
    "    # Learning rate configuration\n",
    "    learning_rate: float = 1e-4\n",
    "    warmup_steps: int = 2000\n",
    "    \n",
    "    # DeepSpeed configuration\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    zero_stage: int = 3\n",
    "\n",
    "def create_deepspeed_config(config: TrainingConfig) -> Dict[str, Any]:\n",
    "    \"\"\"Create DeepSpeed configuration optimized for 2x T4 GPUs\"\"\"\n",
    "    world_size = torch.cuda.device_count()  # Get number of available GPUs\n",
    "    micro_batch = config.train_batch_size // world_size  # Ensure divisible by world_size\n",
    "    gradient_acc_steps = 1\n",
    "    train_batch = micro_batch * gradient_acc_steps * world_size\n",
    "    \n",
    "    print(f\"DeepSpeed Config - world_size: {world_size}, micro_batch: {micro_batch}, train_batch: {train_batch}\")\n",
    "    \n",
    "    return {\n",
    "        \"train_micro_batch_size_per_gpu\": micro_batch,\n",
    "        \"gradient_accumulation_steps\": gradient_acc_steps,\n",
    "        \"train_batch_size\": train_batch,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": config.learning_rate,\n",
    "                \"betas\": (0.9, 0.95),\n",
    "                \"weight_decay\": 0.1,\n",
    "            },\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"WarmupLR\",\n",
    "            \"params\": {\"warmup_num_steps\": config.warmup_steps}\n",
    "        },\n",
    "        \"fp16\": {\"enabled\": config.fp16},\n",
    "        \"bf16\": {\"enabled\": config.bf16},\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": config.zero_stage,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"none\",\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"none\",\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": 5e7,\n",
    "            \"stage3_prefetch_bucket_size\": 5e7,\n",
    "            \"stage3_param_persistence_threshold\": 1e5,\n",
    "        },\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"steps_per_print\": 10,\n",
    "        \"wall_clock_breakdown\": False\n",
    "    }\n",
    "\n",
    "def train_func(config: Dict[str, Any]) -> None:\n",
    "    \"\"\"Training function that will be launched on each worker\"\"\"\n",
    "    \n",
    "    # Initialize distributed backend\n",
    "    deepspeed.init_distributed()\n",
    "    \n",
    "    # Convert dictionary config to TrainingConfig\n",
    "    train_config = TrainingConfig(\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        block_size=config[\"block_size\"],\n",
    "        n_layer=config[\"n_layer\"],\n",
    "        n_head=config[\"n_head\"],\n",
    "        n_embd=config[\"n_embd\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        train_batch_size=config[\"train_batch_size\"],\n",
    "        eval_batch_size=config[\"eval_batch_size\"],\n",
    "        log_interval=config[\"log_interval\"],\n",
    "        data_path=config[\"data_path\"],\n",
    "        tokenizer_name=config[\"tokenizer_name\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        fp16=config[\"fp16\"],\n",
    "        bf16=config[\"bf16\"],\n",
    "        zero_stage=config[\"zero_stage\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize model configuration\n",
    "    model_config = GPTConfig(\n",
    "        vocab_size=train_config.vocab_size,\n",
    "        block_size=train_config.block_size,\n",
    "        n_layer=train_config.n_layer,\n",
    "        n_head=train_config.n_head,\n",
    "        n_embd=train_config.n_embd,\n",
    "        dropout=train_config.dropout\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GPT(model_config)\n",
    "    \n",
    "  \n",
    "    \n",
    "    # Initialize DeepSpeed with proper config\n",
    "    ds_config = create_deepspeed_config(train_config)\n",
    "    model, optimizer, _, scheduler = deepspeed.initialize(\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "        config=ds_config\n",
    "    )\n",
    "    \n",
    "    # Get data shard for this worker and create iterators\n",
    "    train_dataset = ray.train.get_dataset_shard(\"train\")\n",
    "    val_dataset = ray.train.get_dataset_shard(\"validation\")\n",
    "    \n",
    "    train_dataloader = train_dataset.iter_torch_batches(\n",
    "        batch_size=train_config.train_batch_size,\n",
    "        dtypes={\"input_ids\": torch.long, \"labels\": torch.long}\n",
    "    )\n",
    "    val_dataloader = val_dataset.iter_torch_batches(\n",
    "        batch_size=train_config.eval_batch_size,\n",
    "        dtypes={\"input_ids\": torch.long, \"labels\": torch.long}\n",
    "    )\n",
    "    \n",
    "    device = get_accelerator().device_name(model.local_rank)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(train_config.num_epochs):  # Changed from config to train_config\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        # Use the iterator directly\n",
    "        for batch in train_dataloader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            targets = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(input_ids)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            \n",
    "            # Backward pass using DeepSpeed engine\n",
    "            model.backward(loss)\n",
    "            model.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if num_train_batches % train_config.log_interval == 0 and model.global_rank == 0:\n",
    "                print(f\"Epoch: {epoch}, Batch: {num_train_batches}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                targets = batch[\"labels\"].to(device)\n",
    "                \n",
    "                logits, _ = model(input_ids)\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    targets.view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss = train_loss / num_train_batches if num_train_batches > 0 else float('inf')\n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else float('inf')\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        \n",
    "        # Save checkpoint and report metrics\n",
    "        if model.global_rank == 0:\n",
    "            print(f\"Epoch {epoch}: {metrics}\")\n",
    "            \n",
    "        with TemporaryDirectory() as tmpdir:\n",
    "            # Save model checkpoint\n",
    "            model.save_checkpoint(tmpdir)\n",
    "            \n",
    "            # Ensure all workers finished saving\n",
    "            torch.distributed.barrier()\n",
    "            \n",
    "            # Report metrics and checkpoint\n",
    "            ray.train.report(\n",
    "                metrics=metrics,\n",
    "                checkpoint=Checkpoint.from_directory(tmpdir)\n",
    "            )\n",
    "\n",
    "def main():\n",
    "    # Shutdown Ray if it's already running\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    \n",
    "    # Initialize Ray configurations\n",
    "    ray_config = RayConfig()\n",
    "    training_config = TrainingConfig()\n",
    "    \n",
    "    # Initialize Ray cluster\n",
    "    if ray_config.address:\n",
    "        ray.init(address=ray_config.address, ignore_reinit_error=True)\n",
    "    else:\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "    \n",
    "    try:\n",
    "        # Convert training config to dictionary\n",
    "        train_config_dict = {\n",
    "            key: getattr(training_config, key)\n",
    "            for key in training_config.__dataclass_fields__\n",
    "        }\n",
    "        \n",
    "        # Initialize datasets\n",
    "        data_manager = DatasetManager(DatasetConfig(csv_path=training_config.data_path))\n",
    "        data_manager.initialize_datasets()\n",
    "        \n",
    "        # Convert tensors to numpy arrays for Ray Dataset\n",
    "        train_data = [(x.numpy(), y.numpy()) for x, y in data_manager.train_dataset]\n",
    "        val_data = [(x.numpy(), y.numpy()) for x, y in data_manager.val_dataset]\n",
    "        \n",
    "        # Convert to Ray datasets with proper schema\n",
    "        ray_datasets = {\n",
    "            \"train\": ray.data.from_items([\n",
    "                {\"input_ids\": x, \"labels\": y} for x, y in train_data\n",
    "            ]),\n",
    "            \"validation\": ray.data.from_items([\n",
    "                {\"input_ids\": x, \"labels\": y} for x, y in val_data\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = TorchTrainer(\n",
    "            train_func,\n",
    "            train_loop_config=train_config_dict,\n",
    "            scaling_config=ray_config.get_scaling_config(),\n",
    "            datasets=ray_datasets,\n",
    "            dataset_config=DataConfig(\n",
    "                datasets_to_split=[\"train\", \"validation\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        result = trainer.fit()\n",
    "        \n",
    "        # Get best checkpoints\n",
    "        best_checkpoints = result.best_checkpoints\n",
    "        return best_checkpoints\n",
    "        \n",
    "    finally:\n",
    "        # Always shutdown Ray when we're done\n",
    "        ray.shutdown()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ed876",
   "metadata": {
    "papermill": {
     "duration": 0.052396,
     "end_time": "2025-01-20T03:11:43.902502",
     "exception": false,
     "start_time": "2025-01-20T03:11:43.850106",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6173867,
     "sourceId": 10463450,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 588.034227,
   "end_time": "2025-01-20T03:11:46.850926",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-20T03:01:58.816699",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0108a77492f34b1da98feb3b00625870": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3b08934d71b641aca1ad58b384ce898c",
       "placeholder": "​",
       "style": "IPY_MODEL_467095c410b148bcbffc666fceed60a9",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [01:59&lt;00:00, 87.9 row/s]"
      }
     },
     "013eea2db0f9474eb7f1ba2e0b4c6482": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "015a346991ec4a53bc6b255c5986ab10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "06141eece21641bab1574a4624cf4eb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07dd93d357724d08894bc2dcc9169f19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6b290659de9d4c779d113d38115fecc7",
       "placeholder": "​",
       "style": "IPY_MODEL_4154f49356bd4e2db71d1cf440a776a8",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 41.4MB object store; [locality disabled]: "
      }
     },
     "0c29e9fb0cc042b7b105f08c714dbeab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_baa144225242480b84f571cb88786733",
       "placeholder": "​",
       "style": "IPY_MODEL_3e2461d19c9745078e67ee63eea46466",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 14.0MB object store; [locality disabled]: "
      }
     },
     "0d5c2be5520b4be0bc37c46e9f99eaea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f1dfc0c051442dc9f3edd28ffcd6bae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0c29e9fb0cc042b7b105f08c714dbeab",
        "IPY_MODEL_cc2276eb6adb48dfbbf9759e8f16a2a0",
        "IPY_MODEL_3aaca9db4af44f36bfaa30d937f3c09a"
       ],
       "layout": "IPY_MODEL_f9ca20269672448881aaf18b17e22289",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0f75031e2ee94999a2e875aaa0936b71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1121c807fbe14ff095ebcf8e919dfab9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "115668c68e914771adc0b174f6229024": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_652be61d405f4e41b29cc3093581d715",
        "IPY_MODEL_9d2edafc51e74cf4a3bbf2ce97e4fd2a",
        "IPY_MODEL_b8b8890dd3034995a68f32fc5db63e98"
       ],
       "layout": "IPY_MODEL_1736eab6a07b4f409bd9d60cec026896",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1635fc11b6cd486ba2b9f2049ab1b68b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_798a7338b5454178a63cffedbe9efb5f",
       "placeholder": "​",
       "style": "IPY_MODEL_94283135c16e4aa2883f548471f63d4b",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 312 row/s]"
      }
     },
     "171c90ada4f74fb6be28bd6018bd6445": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1736eab6a07b4f409bd9d60cec026896": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "232e90423d0b4f0eb8e3c3d12b71ec4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24007b0699ee4216951867a62623054d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_433f1664c3614834a8058058b8423700",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dbd4bac53e4a4765803f138d9b59d710",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "2623103e1d4644adb51c8e94359c4680": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "265a540a2e1c4ac1a84203999d0e97fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2709c28664e34e338ed8008781c4fdb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_06141eece21641bab1574a4624cf4eb6",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_846c0ac00de040dc9a7f8c05a5183d42",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "29cd676f351049cab195df49b21ef1e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a17e2fc3c8ad40f0ba653dd446a9e739",
       "placeholder": "​",
       "style": "IPY_MODEL_e8b1421f6dab46e8a7eac68ea6a31453",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) ✔️  Dataset execution finished in 120.10 seconds: "
      }
     },
     "308ea5aeddee4d71870df29da0b5578f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07dd93d357724d08894bc2dcc9169f19",
        "IPY_MODEL_435f7635dbae4c2682f9840c633a7f0b",
        "IPY_MODEL_0108a77492f34b1da98feb3b00625870"
       ],
       "layout": "IPY_MODEL_49f249a88cbd4efa83d279e46896bc8b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "310f32e2794a49c38fcd2d3658271c75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_013eea2db0f9474eb7f1ba2e0b4c6482",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffc29c23270744e7885e51608054adff",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "31321c9244aa455b8c296a48dbdc8d16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31b07bb004014e79a5596a5702e73a08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b92be13740074b5a927a40b4c3356e5a",
       "placeholder": "​",
       "style": "IPY_MODEL_015a346991ec4a53bc6b255c5986ab10",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) ✔️  Dataset execution finished in 124.33 seconds: "
      }
     },
     "35fe0d9abc474f32b029ce80890732ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7ce1f4718afd4cf5b8368ca3d7b40eca",
       "placeholder": "​",
       "style": "IPY_MODEL_c4f85a9e2a3545bc80908fcd5eb02871",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 14.0MB object store; [locality disabled]: "
      }
     },
     "39ea1f56b16e4a84abf904c9ae5be4f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "3a1455216e02497bb4a37e682d8fa481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3aaca9db4af44f36bfaa30d937f3c09a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5e95c56359c94267bccc6549875bc6df",
       "placeholder": "​",
       "style": "IPY_MODEL_b2c319306e304a1b96dfaaa82646d325",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 312 row/s]"
      }
     },
     "3b08934d71b641aca1ad58b384ce898c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c20540f29e94e2e859825f333fd1334": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e2461d19c9745078e67ee63eea46466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4154f49356bd4e2db71d1cf440a776a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "433f1664c3614834a8058058b8423700": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "435f7635dbae4c2682f9840c633a7f0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b6bd73e4e31c4e869625229b36e433ac",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_55451acf4b764924b44b5353c986c673",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "4391e5f7fc0e4d7884ac3fbc8675c4f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "440ef5de0be041d7b736d054b73b0926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0d5c2be5520b4be0bc37c46e9f99eaea",
       "placeholder": "​",
       "style": "IPY_MODEL_84e49a42dd3c41b5aed5b9744498135c",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) ✔️  Dataset execution finished in 124.73 seconds: "
      }
     },
     "467095c410b148bcbffc666fceed60a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "467ebd5bb3da42c8bc6938f416e2a687": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1121c807fbe14ff095ebcf8e919dfab9",
       "placeholder": "​",
       "style": "IPY_MODEL_dce9f7c979ab4d198da40a90a86ab1c9",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 283 row/s]"
      }
     },
     "4927b3a5b58e4a6495e2b271f341069b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "49f249a88cbd4efa83d279e46896bc8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "4f0e7280a75e4fdead9cd28b154fc972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "51a11ccff3a84c6e84c621364c892442": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "52acc611b3fb47879213b8ef47c4eb29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d212998753004a93bb534337f31a9f1c",
       "placeholder": "​",
       "style": "IPY_MODEL_a9ba09d3594f4778a2981c32801a732d",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) ✔️  Dataset execution finished in 11.42 seconds: "
      }
     },
     "54764416be004f8c84213092b01dfff9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3c20540f29e94e2e859825f333fd1334",
       "placeholder": "​",
       "style": "IPY_MODEL_8f17fc7ddc014270a739e8a4e3a67f51",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [02:04&lt;00:00, 69.3 row/s]"
      }
     },
     "55451acf4b764924b44b5353c986c673": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "561c066dafc74723b6cdbc0c1463197c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59b8818f9ff944ccbb2a24a1b4653b66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5bc060d8da794104b531a18add9a85c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_171c90ada4f74fb6be28bd6018bd6445",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_724561e148b14294885b05df57a29389",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "5e95c56359c94267bccc6549875bc6df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "652be61d405f4e41b29cc3093581d715": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b65a8b942fab4cbca10078242c6a6624",
       "placeholder": "​",
       "style": "IPY_MODEL_9f1685b4e28648848b9ba1bc49aca281",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) ✔️  Dataset execution finished in 11.41 seconds: "
      }
     },
     "6537175cc226422f9f84225f776a3bdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7d8ecdd346ed4173b63d42ea992f990f",
        "IPY_MODEL_5bc060d8da794104b531a18add9a85c2",
        "IPY_MODEL_467ebd5bb3da42c8bc6938f416e2a687"
       ],
       "layout": "IPY_MODEL_b3769e3b60bf40fdb9e6c458a5c587bf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "68d3eb47e80e49809e621b1f0cc6f487": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b290659de9d4c779d113d38115fecc7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b5605016ab14a49996375832932ed4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6cf569b5ed804d13b9f306573e813296": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7124d797067246acaf209569b1cc54e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "724561e148b14294885b05df57a29389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "74ed1cd108ee463b8e1c9c8faf347861": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a82b6297753c42b8a790d5d738ce3044",
       "placeholder": "​",
       "style": "IPY_MODEL_f77b8623d2a34c659e6942368abd381e",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 41.4MB object store; [locality disabled]: "
      }
     },
     "7928c54fc4274440aaec7136cbc7367a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e844e2c357d4a12b10bae3f5277f40f",
       "placeholder": "​",
       "style": "IPY_MODEL_ab691fcb4dcf4b12816ef22ed96f5906",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=635) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 41.4MB object store; [locality disabled]: "
      }
     },
     "798a7338b5454178a63cffedbe9efb5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b2ecb1a4bd642e196482928c36ae414": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf3e6d58e6e64f1bb2b731d77b800efa",
       "placeholder": "​",
       "style": "IPY_MODEL_a99e9444a3164a979421fd77b0f5a317",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [02:04&lt;00:00, 84.6 row/s]"
      }
     },
     "7ce1f4718afd4cf5b8368ca3d7b40eca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d8ecdd346ed4173b63d42ea992f990f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c89f4fce1acb4a60bfc70a5465a3ea8a",
       "placeholder": "​",
       "style": "IPY_MODEL_59b8818f9ff944ccbb2a24a1b4653b66",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) ✔️  Dataset execution finished in 11.49 seconds: "
      }
     },
     "7e844e2c357d4a12b10bae3f5277f40f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8247cec1b21f4e6f9bd84992b1d6f2ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83adc2236aba4d65be8a5c992152c65c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f75031e2ee94999a2e875aaa0936b71",
       "placeholder": "​",
       "style": "IPY_MODEL_561c066dafc74723b6cdbc0c1463197c",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 281 row/s]"
      }
     },
     "846c0ac00de040dc9a7f8c05a5183d42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "84e49a42dd3c41b5aed5b9744498135c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "875dbbde68ef4881bc4fad4c19c6ae35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f17fc7ddc014270a739e8a4e3a67f51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94283135c16e4aa2883f548471f63d4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95adb6426f8a49928f95b402e05b8ad7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "975d6c3213f84834b83a5756433c321f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2623103e1d4644adb51c8e94359c4680",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6e7be5d0064454ead97b559f5ce86ae",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "995c09b2a5ca4b9f84413bd0ca58edef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ae1d43eca9c4e2886f17956b7bed239": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7928c54fc4274440aaec7136cbc7367a",
        "IPY_MODEL_bf5cd18f95cc43928fb8ab8afd4f6783",
        "IPY_MODEL_7b2ecb1a4bd642e196482928c36ae414"
       ],
       "layout": "IPY_MODEL_f699801f6f2c497ab7388bcc05ad0080",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9d1f5856fd714913b1091c31e6d808e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_232e90423d0b4f0eb8e3c3d12b71ec4f",
       "placeholder": "​",
       "style": "IPY_MODEL_d2489a99efdf4d65ab101efce0a6477e",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 313 row/s]"
      }
     },
     "9d2edafc51e74cf4a3bbf2ce97e4fd2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cba3237901244841ad0eb35c35119cf2",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4f0e7280a75e4fdead9cd28b154fc972",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "9f1685b4e28648848b9ba1bc49aca281": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a17e2fc3c8ad40f0ba653dd446a9e739": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7e28354604c4ea6a90786c6d156b76e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_440ef5de0be041d7b736d054b73b0926",
        "IPY_MODEL_975d6c3213f84834b83a5756433c321f",
        "IPY_MODEL_54764416be004f8c84213092b01dfff9"
       ],
       "layout": "IPY_MODEL_39ea1f56b16e4a84abf904c9ae5be4f8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a82b6297753c42b8a790d5d738ce3044": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a99e9444a3164a979421fd77b0f5a317": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9ba09d3594f4778a2981c32801a732d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aa147d2b25fd4bdb9775bbd83540ba99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab691fcb4dcf4b12816ef22ed96f5906": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "adb60d5470b644c39f0bdd10a2dca2b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b0c08941d5554c1e9cdbd8082146fce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b2c319306e304a1b96dfaaa82646d325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b3769e3b60bf40fdb9e6c458a5c587bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "b3e939810b4044d2af46c70d1d544026": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b521c17d9b4c49f49a46f346433c95bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6b5605016ab14a49996375832932ed4b",
       "placeholder": "​",
       "style": "IPY_MODEL_c7dda778687b4a11969bbb8f7c3d1888",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [02:04&lt;00:00, 84.9 row/s]"
      }
     },
     "b65a8b942fab4cbca10078242c6a6624": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6bd73e4e31c4e869625229b36e433ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8b8890dd3034995a68f32fc5db63e98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8247cec1b21f4e6f9bd84992b1d6f2ce",
       "placeholder": "​",
       "style": "IPY_MODEL_7124d797067246acaf209569b1cc54e6",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.56k/? [00:11&lt;00:00, 274 row/s]"
      }
     },
     "b92be13740074b5a927a40b4c3356e5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "baa144225242480b84f571cb88786733": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc130245a4974a4ea521d25c58cd5203": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be958a6acdf44c05b6abde541e10172c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ec9eb252bfbb43dab154c59f008fd765",
        "IPY_MODEL_310f32e2794a49c38fcd2d3658271c75",
        "IPY_MODEL_1635fc11b6cd486ba2b9f2049ab1b68b"
       ],
       "layout": "IPY_MODEL_d140c2b678db448eb18c1ce291ffc4d0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "bf3e6d58e6e64f1bb2b731d77b800efa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf5cd18f95cc43928fb8ab8afd4f6783": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_265a540a2e1c4ac1a84203999d0e97fd",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6cf569b5ed804d13b9f306573e813296",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "c0e91c94d2bd46399ade04f7ec0b7cde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "c4f85a9e2a3545bc80908fcd5eb02871": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c7dda778687b4a11969bbb8f7c3d1888": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c89f4fce1acb4a60bfc70a5465a3ea8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca2ac1c8ac974555b23dfe20d8fc266f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0c08941d5554c1e9cdbd8082146fce0",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4927b3a5b58e4a6495e2b271f341069b",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "ca6e0d7f1cab49268db43841bcea63ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4391e5f7fc0e4d7884ac3fbc8675c4f0",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_adb60d5470b644c39f0bdd10a2dca2b4",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "cba3237901244841ad0eb35c35119cf2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc2276eb6adb48dfbbf9759e8f16a2a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff04817b9d7a4fafa0d8d043ffd6999d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3e939810b4044d2af46c70d1d544026",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "ccc8b660b5b549d6ae0eb45e0c73c3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d056c4b14d964d22a7e2dd9d6f600af4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_74ed1cd108ee463b8e1c9c8faf347861",
        "IPY_MODEL_2709c28664e34e338ed8008781c4fdb7",
        "IPY_MODEL_b521c17d9b4c49f49a46f346433c95bc"
       ],
       "layout": "IPY_MODEL_df5efe15cf5e4a0c93d6d80d8bd6f43c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d140c2b678db448eb18c1ce291ffc4d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "d212998753004a93bb534337f31a9f1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2489a99efdf4d65ab101efce0a6477e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dbd4bac53e4a4765803f138d9b59d710": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dc736dd9076b4557bc7863c987e28a33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_995c09b2a5ca4b9f84413bd0ca58edef",
       "placeholder": "​",
       "style": "IPY_MODEL_875dbbde68ef4881bc4fad4c19c6ae35",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [02:00&lt;00:00, 66.9 row/s]"
      }
     },
     "dce9f7c979ab4d198da40a90a86ab1c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "df5efe15cf5e4a0c93d6d80d8bd6f43c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "e375a4c1cd78484fb065beb60ecc2de2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "e53f93b34fe34420a031b945bf04476d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bc130245a4974a4ea521d25c58cd5203",
       "placeholder": "​",
       "style": "IPY_MODEL_68d3eb47e80e49809e621b1f0cc6f487",
       "tabbable": null,
       "tooltip": null,
       "value": " 10.6k/? [02:04&lt;00:00, 69.1 row/s]"
      }
     },
     "e6a45a03098b41a886cead533ff7a99d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_35fe0d9abc474f32b029ce80890732ea",
        "IPY_MODEL_ca6e0d7f1cab49268db43841bcea63ea",
        "IPY_MODEL_9d1f5856fd714913b1091c31e6d808e0"
       ],
       "layout": "IPY_MODEL_c0e91c94d2bd46399ade04f7ec0b7cde",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e6e7be5d0064454ead97b559f5ce86ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e8b1421f6dab46e8a7eac68ea6a31453": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ec9eb252bfbb43dab154c59f008fd765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_31321c9244aa455b8c296a48dbdc8d16",
       "placeholder": "​",
       "style": "IPY_MODEL_ccc8b660b5b549d6ae0eb45e0c73c3b9",
       "tabbable": null,
       "tooltip": null,
       "value": "(pid=636) - split(2, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 14.0MB object store; [locality disabled]: "
      }
     },
     "ed366589199644919cd79ce943b26559": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_29cd676f351049cab195df49b21ef1e2",
        "IPY_MODEL_ca2ac1c8ac974555b23dfe20d8fc266f",
        "IPY_MODEL_dc736dd9076b4557bc7863c987e28a33"
       ],
       "layout": "IPY_MODEL_e375a4c1cd78484fb065beb60ecc2de2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f0041a0353e34ec5be810f1b3d99d25b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_52acc611b3fb47879213b8ef47c4eb29",
        "IPY_MODEL_f31941690f314450ace0abfdd8cc5b2b",
        "IPY_MODEL_83adc2236aba4d65be8a5c992152c65c"
       ],
       "layout": "IPY_MODEL_95adb6426f8a49928f95b402e05b8ad7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f31941690f314450ace0abfdd8cc5b2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aa147d2b25fd4bdb9775bbd83540ba99",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a1455216e02497bb4a37e682d8fa481",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "f699801f6f2c497ab7388bcc05ad0080": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "f77b8623d2a34c659e6942368abd381e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9ca20269672448881aaf18b17e22289": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "fe885e8b6a534ac3bce829f8fbcf1841": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31b07bb004014e79a5596a5702e73a08",
        "IPY_MODEL_24007b0699ee4216951867a62623054d",
        "IPY_MODEL_e53f93b34fe34420a031b945bf04476d"
       ],
       "layout": "IPY_MODEL_51a11ccff3a84c6e84c621364c892442",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ff04817b9d7a4fafa0d8d043ffd6999d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffc29c23270744e7885e51608054adff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
